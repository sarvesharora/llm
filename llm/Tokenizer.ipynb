{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed8b60c-3c6f-4240-bdc0-8d7dace59833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\") # ord function gives you unicode code points, don't support strings for any languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630817e1-07ee-41c7-8a1e-ef8f8d8f14bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"hello how are you\".encode(\"utf-8\")) #sequence will be to long to feed it to llm, will hinder the context of the llm\n",
    "# so we come to Byte pair encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b9c180-d42b-476d-92f6-d7a05fbebab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Pair encoding is just count the pair of bytes with most occourences and assign them to new token\n",
    "# this is just to shorten the encoding that we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82eba62-cd66-44da-9bb6-40e8c130362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, 世界! こんにちは世界。你好，世界! नमस्ते, दुनिया! नमस्ते दुनिया! This text includes English, Chinese, Japanese, and Hindi characters.\n",
      "-----\n",
      "[72, 101, 108, 108, 111, 44, 32, 228, 184, 150, 231, 149, 140, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 228, 184, 150, 231, 149, 140, 227, 128, 130, 228, 189, 160, 229, 165, 189, 239, 188, 140, 228, 184, 150, 231, 149, 140, 33, 32, 224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135, 44, 32, 224, 164, 166, 224, 165, 129, 224, 164, 168, 224, 164, 191, 224, 164, 175, 224, 164, 190, 33, 32, 224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135, 32, 224, 164, 166, 224, 165, 129, 224, 164, 168, 224, 164, 191, 224, 164, 175, 224, 164, 190, 33, 32, 84, 104, 105, 115, 32, 116, 101, 120, 116, 32, 105, 110, 99, 108, 117, 100, 101, 115, 32, 69, 110, 103, 108, 105, 115, 104, 44, 32, 67, 104, 105, 110, 101, 115, 101, 44, 32, 74, 97, 112, 97, 110, 101, 115, 101, 44, 32, 97, 110, 100, 32, 72, 105, 110, 100, 105, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 46]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, 世界! こんにちは世界。你好，世界! नमस्ते, दुनिया! नमस्ते दुनिया! This text includes English, Chinese, Japanese, and Hindi characters.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(text)\n",
    "print(\"-----\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0446b0-4231-445a-b66b-90da75635c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmostcommonpair(arr):\n",
    "    print(len(arr))\n",
    "    freq = dict()\n",
    "    ans = (arr[0],arr[1])\n",
    "    cnt = 0\n",
    "    for i in range(len(arr)-1):\n",
    "        key = (arr[i],arr[i+1])\n",
    "        if (key in freq):\n",
    "            freq[key] += 1\n",
    "        else:\n",
    "            freq[key] = 1\n",
    "\n",
    "    return max(freq, key = freq.get) # gets you the max freq key\n",
    "    \n",
    "#mostfreq = findmostcommonpair(tokens)\n",
    "#mostfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bf004c-078c-4027-b414-040ba00950e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(arr, maxkey, replaceby):\n",
    "    newarr = []\n",
    "    i = 0\n",
    "    while(i < len(arr)):\n",
    "        if(i==len(arr)-1):\n",
    "            newarr.append(arr[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            if(arr[i]==maxkey[0] and arr[i+1]==maxkey[1]):\n",
    "                newarr.append(replaceby)\n",
    "                i+=2\n",
    "            else:\n",
    "                newarr.append(arr[i])\n",
    "                i+=1\n",
    "    return newarr\n",
    "\n",
    "#tokens2 = merge(tokens, mostfreq, 256)\n",
    "#print(len(tokens), len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c63c5f-a4d6-4181-83dc-421e2b382ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "going to merge (224, 164) to 257\n",
      "185\n",
      "going to merge (224, 165) to 258\n",
      "179\n",
      "going to merge (44, 32) to 259\n",
      "174\n",
      "going to merge (33, 32) to 260\n",
      "170\n",
      "going to merge (227, 129) to 261\n",
      "166\n",
      "going to merge (257, 168) to 262\n",
      "162\n",
      "going to merge (262, 257) to 263\n",
      "158\n",
      "going to merge (228, 184) to 264\n",
      "155\n",
      "going to merge (264, 150) to 265\n",
      "152\n",
      "going to merge (265, 231) to 266\n",
      "149\n",
      "going to merge (266, 149) to 267\n",
      "146\n",
      "going to merge (267, 140) to 268\n",
      "143\n",
      "going to merge (105, 110) to 269\n",
      "140\n",
      "going to merge (101, 115) to 270\n",
      "137\n",
      "going to merge (268, 260) to 271\n",
      "135\n",
      "going to merge (263, 174) to 272\n",
      "133\n",
      "going to merge (272, 257) to 273\n",
      "131\n",
      "going to merge (273, 184) to 274\n",
      "129\n",
      "going to merge (274, 258) to 275\n",
      "127\n",
      "going to merge (275, 141) to 276\n",
      "125 203\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276 #homuch big vacab you want\n",
    "num_merges = vocab_size - 256 #how much you want to merge, for every merge operation we get a new token\n",
    "\n",
    "i = 1\n",
    "merges = {}\n",
    "reversemerges = {}\n",
    "originalarr = list(tokens)\n",
    "while(i <= num_merges):\n",
    "    newtoken = 256+i\n",
    "    most_freq = findmostcommonpair(tokens)\n",
    "    merges[most_freq] = newtoken\n",
    "    reversemerges[newtoken] = most_freq\n",
    "    print(f\"going to merge {most_freq} to {newtoken}\")\n",
    "    tokens = merge(tokens, most_freq, newtoken)\n",
    "    i+=1\n",
    "\n",
    "print(len(tokens), len(originalarr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6ea7913-4b51-4679-a787-2bc7480cf248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस�'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given [257, 256] turn into seq []\n",
    "\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for key in merges:\n",
    "    vocab[merges[key]] = vocab[key[0]] + vocab[key[1]]\n",
    "\n",
    "\n",
    "# def decodesingleint(ids):\n",
    "#     if ids < 256:\n",
    "#         return list((ids,))\n",
    "\n",
    "#     tup = reversemerges[ids]\n",
    "#     l1 = decodesingleint(tup[0])\n",
    "#     l2 = decodesingleint(tup[1])\n",
    "    \n",
    "#     l1+=l2\n",
    "#     return l1\n",
    "\n",
    "# def decode2(ids):\n",
    "#     newarr = []\n",
    "#     encodings = {}\n",
    "#     for i in range(256):\n",
    "#         encodings[i] = chr(i)\n",
    "\n",
    "#     for i in ids:\n",
    "#         newarr += decodesingleint(i)\n",
    "#     st = \"\"\n",
    "#     for i in newarr:\n",
    "#         st+=encodings[i]\n",
    "        \n",
    "#     return st\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors = 'replace')\n",
    "\n",
    "    return text\n",
    "    \n",
    "decode([275])\n",
    "# decode2([275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2ff8a31-7fd3-4169-97a1-4cae6ec1aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text includes English, Chinese, Japanese, and Hindi characters.\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    for key in merges:\n",
    "        newarr = []\n",
    "        i = 0\n",
    "        while i < (len(tokens)):\n",
    "            if(i+1<len(tokens) and tokens[i]==key[0] and tokens[i+1]==key[1]):\n",
    "                newarr.append(merges[key])\n",
    "                i+=2\n",
    "            else:\n",
    "                newarr.append(tokens[i])\n",
    "                i+=1\n",
    "        tokens = newarr\n",
    "        \n",
    "    return tokens\n",
    "        \n",
    "print(decode(encode(\"This text includes English, Chinese, Japanese, and Hindi characters.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06496b-08c2-41f4-add5-2ef370caa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## video link https://www.youtube.com/watch?v=zduSFxRajkE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
